{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s09_IC6DCjD0"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------\n",
        "# Cell 1: Install & Import Libraries\n",
        "# ---------------------------------\n",
        "\n",
        "!pip install opendatasets --quiet\n",
        "\n",
        "import opendatasets as od\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# Cell 1.5: Mount Google Drive\n",
        "# ---------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted!\")"
      ],
      "metadata": {
        "id": "ZHyHHCdtQdn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3eb649b-4b4c-4b36-990d-af1c76f21736"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Cell 2: Download Dataset & Define Constants\n",
        "# -------------------------------------------\n",
        "\n",
        "# Download the dataset\n",
        "# This will prompt for your Kaggle username and API key\n",
        "print(\"Please enter your Kaggle credentials:\")\n",
        "dataset_url = 'https://www.kaggle.com/datasets/almightyj/person-face-sketches'\n",
        "od.download(dataset_url)\n",
        "\n",
        "# Define paths and constants\n",
        "DATA_DIR = './person-face-sketches/'\n",
        "PHOTO_DIR = os.path.join(DATA_DIR, 'photos')\n",
        "SKETCH_DIR = os.path.join(DATA_DIR, 'sketches')\n",
        "\n",
        "# Model parameters\n",
        "BUFFER_SIZE = 1000  # For shuffling\n",
        "BATCH_SIZE = 1      # As per the CycleGAN paper\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "OUTPUT_CHANNELS = 3\n",
        "LAMBDA_CYCLE = 10.0\n",
        "LAMBDA_IDENTITY = 0.5 * LAMBDA_CYCLE"
      ],
      "metadata": {
        "id": "ULWCTtAAD3Fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282a7285-45a3-4f9e-9ada-d12e5811237c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your Kaggle credentials:\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: munjenko\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/almightyj/person-face-sketches\n",
            "Downloading person-face-sketches.zip to ./person-face-sketches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.29G/1.29G [00:15<00:00, 90.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Cell 2.5 (Final Version): Set Correct Paths and Verify\n",
        "# -----------------------------------------------\n",
        "import os\n",
        "\n",
        "# This is the main folder you downloaded\n",
        "DATA_DIR = './person-face-sketches/'\n",
        "\n",
        "# --- THIS IS THE FIX ---\n",
        "# The actual image folders are inside the 'train' directory.\n",
        "PHOTO_DIR = os.path.join(DATA_DIR, 'train', 'photos')\n",
        "SKETCH_DIR = os.path.join(DATA_DIR, 'train', 'sketches')\n",
        "# ---------------------\n",
        "\n",
        "print(f\"Corrected PHOTO_DIR path: {PHOTO_DIR}\")\n",
        "print(f\"Corrected SKETCH_DIR path: {SKETCH_DIR}\")\n",
        "\n",
        "print(\"\\n--- Verifying Corrected Paths ---\")\n",
        "\n",
        "if os.path.exists(PHOTO_DIR) and os.path.exists(SKETCH_DIR):\n",
        "    # Count files in the correct 'photos' directory\n",
        "    !echo \"Counting photos in 'train/photos'...\"\n",
        "    !find {PHOTO_DIR} -type f -name \"*.jpg\" | wc -l\n",
        "\n",
        "    # Count files in the correct 'sketches' directory\n",
        "    !echo \"Counting sketches in 'train/sketches'...\"\n",
        "    !find {SKETCH_DIR} -type f -name \"*.jpg\" | wc -l\n",
        "\n",
        "    print(\"\\nVerification complete. You should now see large file counts (e.g., 4000+) for both.\")\n",
        "\n",
        "else:\n",
        "    print(f\"ERROR: Still cannot find directories.\")\n",
        "    print(f\"Checked for photos at: {PHOTO_DIR}\")\n",
        "    print(f\"Checked for sketches at: {SKETCH_DIR}\")\n",
        "    print(\"Please double-check the folder names.\")"
      ],
      "metadata": {
        "id": "ODMy9vx8FXOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d10027-11c4-440b-80a6-9334d10fbebc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected PHOTO_DIR path: ./person-face-sketches/train/photos\n",
            "Corrected SKETCH_DIR path: ./person-face-sketches/train/sketches\n",
            "\n",
            "--- Verifying Corrected Paths ---\n",
            "Counting photos in 'train/photos'...\n",
            "20655\n",
            "Counting sketches in 'train/sketches'...\n",
            "20655\n",
            "\n",
            "Verification complete. You should now see large file counts (e.g., 4000+) for both.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------\n",
        "# Cell 3: Preprocessing Functions\n",
        "# -----------------------------------\n",
        "\n",
        "def load_image(image_file):\n",
        "    image = tf.io.read_file(image_file)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    return image\n",
        "\n",
        "def resize(image, height, width):\n",
        "    image = tf.image.resize(image, [height, width],\n",
        "                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    return image\n",
        "\n",
        "# Normalize images to [-1, 1]\n",
        "def normalize(image):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1\n",
        "    return image\n",
        "\n",
        "# --- Load and Preprocess for Training ---\n",
        "def load_image_train(image_file):\n",
        "    image = load_image(image_file)\n",
        "    # Simple data augmentation: random flipping\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = resize(image, IMG_HEIGHT, IMG_WIDTH)\n",
        "    image = normalize(image)\n",
        "    return image\n",
        "\n",
        "# --- Load and Preprocess for Testing (if needed) ---\n",
        "def load_image_test(image_file):\n",
        "    image = load_image(image_file)\n",
        "    image = resize(image, IMG_HEIGHT, IMG_WIDTH)\n",
        "    image = normalize(image)\n",
        "    return image"
      ],
      "metadata": {
        "id": "UpOrgy-GG4ER"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# Cell 4 (New Version): Create Half-Size Dataset\n",
        "# ---------------------------------------------\n",
        "\n",
        "print(\"Creating training datasets...\")\n",
        "\n",
        "# Create datasets by listing all files\n",
        "train_photos_full = tf.data.Dataset.list_files(PHOTO_DIR + '/*.jpg', shuffle=False)\n",
        "train_sketches_full = tf.data.Dataset.list_files(SKETCH_DIR + '/*.jpg', shuffle=False)\n",
        "\n",
        "# --- THIS IS THE CHANGE ---\n",
        "# Get the size (from your verification step) and calculate half\n",
        "dataset_size = 4000\n",
        "half_size = 4000\n",
        "\n",
        "print(f\"Original dataset size found: {dataset_size}\")\n",
        "print(f\"Using new half-size: {half_size}\")\n",
        "\n",
        "# Take only the first half of the datasets\n",
        "train_photos = train_photos_full.take(half_size)\n",
        "train_sketches = train_sketches_full.take(half_size)\n",
        "# --------------------------\n",
        "\n",
        "# Now, process (map) only the half-size datasets\n",
        "train_photos = train_photos.map(load_image_train,\n",
        "                                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_photos = train_photos.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_sketches = train_sketches.map(load_image_train,\n",
        "                                   num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_sketches = train_sketches.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Zip the two datasets to train them together\n",
        "train_dataset = tf.data.Dataset.zip((train_photos, train_sketches))\n",
        "\n",
        "print(\"\\nHalf-size datasets created successfully.\")\n",
        "print(f\"New train dataset: {train_dataset}\")"
      ],
      "metadata": {
        "id": "bCHuxPKTG4-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6eb177-8dda-43fa-e385-a275923ddbd7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training datasets...\n",
            "Original dataset size found: 4000\n",
            "Using new half-size: 4000\n",
            "\n",
            "Half-size datasets created successfully.\n",
            "New train dataset: <_ZipDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# Cell 5: Model Architecture (Corrected)\n",
        "# -----------------------------------------\n",
        "\n",
        "# FIX: Replace InstanceNormalization with GroupNormalization\n",
        "def instance_norm():\n",
        "    \"\"\"Returns a GroupNormalization layer functioning as InstanceNormalization.\"\"\"\n",
        "    return layers.GroupNormalization(groups=-1, epsilon=1e-5)\n",
        "\n",
        "def downsample(filters, size, apply_norm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_norm:\n",
        "        # --- THIS LINE IS CHANGED ---\n",
        "        result.add(instance_norm())\n",
        "    result.add(layers.LeakyReLU())\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                                     kernel_initializer=initializer, use_bias=False))\n",
        "    # --- THIS LINE IS CHANGED ---\n",
        "    result.add(instance_norm())\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "    result.add(layers.ReLU())\n",
        "    return result\n",
        "\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, 3])\n",
        "\n",
        "    # Downsampling\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_norm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4),                 # (bs, 64, 64, 128)\n",
        "        downsample(256, 4),                 # (bs, 32, 32, 256)\n",
        "        downsample(512, 4),                 # (bs, 16, 16, 512)\n",
        "        downsample(512, 4),                 # (bs, 8, 8, 512)\n",
        "        downsample(512, 4),                 # (bs, 4, 4, 512)\n",
        "        downsample(512, 4),                 # (bs, 2, 2, 512)\n",
        "        downsample(512, 4),                 # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    # Upsampling\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "        upsample(512, 4),                     # (bs, 16, 16, 1024)\n",
        "        upsample(256, 4),                     # (bs, 32, 32, 512)\n",
        "        upsample(128, 4),                     # (bs, 64, 64, 256)\n",
        "        upsample(64, 4),                      # (bs, 128, 128, 128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                  strides=2,\n",
        "                                  padding='same',\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  activation='tanh') # Output is [-1, 1]\n",
        "\n",
        "    x = inputs\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "def build_discriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    inp = layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, 3], name='input_image')\n",
        "\n",
        "    down1 = downsample(64, 4, False)(inp) # (bs, 128, 128, 64)\n",
        "    down2 = downsample(128, 4)(down1)     # (bs, 64, 64, 128)\n",
        "    down3 = downsample(256, 4)(down2)     # (bs, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "    conv = layers.Conv2D(512, 4, strides=1,\n",
        "                         kernel_initializer=initializer,\n",
        "                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "    # --- THIS LINE IS CHANGED ---\n",
        "    norm1 = instance_norm()(conv)\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "    last = layers.Conv2D(1, 4, strides=1,\n",
        "                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)\n",
        "\n",
        "print(\"Model architecture defined.\")"
      ],
      "metadata": {
        "id": "CzNsVe14HP1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16fece1f-3b70-49d0-ed6c-a667a40619f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model architecture defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Cell 6: Build Models, Losses, & Optimizers\n",
        "# -----------------------------------------------\n",
        "\n",
        "# Instantiate the models\n",
        "gen_G = build_generator() # Photo -> Sketch\n",
        "gen_F = build_generator() # Sketch -> Photo\n",
        "\n",
        "disc_X = build_discriminator() # Discriminates Photos (Domain A)\n",
        "disc_Y = build_discriminator() # Discriminates Sketches (Domain B)\n",
        "\n",
        "print(\"Models instantiated.\")\n",
        "\n",
        "# --- Loss Functions ---\n",
        "# We use Mean Squared Error for LSGAN loss, which is more stable\n",
        "loss_obj = tf.keras.losses.MeanSquaredError()\n",
        "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "def discriminator_loss(real, generated):\n",
        "    real_loss = loss_obj(tf.ones_like(real), real)\n",
        "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "    total_disc_loss = (real_loss + generated_loss) * 0.5\n",
        "    return total_disc_loss\n",
        "\n",
        "def generator_loss(generated):\n",
        "    # The generator wants the discriminator to think the fake image is real\n",
        "    return loss_obj(tf.ones_like(generated), generated)\n",
        "\n",
        "def cycle_loss(real_image, cycled_image):\n",
        "    loss = mae_loss(real_image, cycled_image)\n",
        "    return LAMBDA_CYCLE * loss\n",
        "\n",
        "def identity_loss(real_image, same_image):\n",
        "    loss = mae_loss(real_image, same_image)\n",
        "    return LAMBDA_IDENTITY * loss\n",
        "\n",
        "# --- Optimizers ---\n",
        "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "print(\"Losses and optimizers defined.\")"
      ],
      "metadata": {
        "id": "hKMSfzR7HhGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef411e4-2a6e-4d69-be00-008c562c8134"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models instantiated.\n",
            "Losses and optimizers defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# Cell 7 (New Version): Checkpoint Setup with Google Drive\n",
        "# ---------------------------------\n",
        "\n",
        "# --- THIS IS THE CHANGE ---\n",
        "# We point the path to a folder in your Google Drive\n",
        "checkpoint_path = \"/content/drive/MyDrive/Colab_CheckPoints/CycleGAN/train\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "!mkdir -p {checkpoint_path}\n",
        "# --------------------------\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator_g=gen_G,\n",
        "                           generator_f=gen_F,\n",
        "                           discriminator_x=disc_X,\n",
        "                           discriminator_y=disc_Y,\n",
        "                           generator_g_optimizer=generator_g_optimizer,\n",
        "                           generator_f_optimizer=generator_f_optimizer,\n",
        "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
        "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# Restore from last checkpoint if it exists\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print (f'Latest checkpoint restored from {checkpoint_path}!')\n",
        "else:\n",
        "    print('No checkpoint found. Starting from scratch.')"
      ],
      "metadata": {
        "id": "haVftyQmHilL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880d34bd-18d9-4e32-8312-79e8fe2c1fda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest checkpoint restored from /content/drive/MyDrive/Colab_CheckPoints/CycleGAN/train!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------\n",
        "# Cell 8: The train_step Function\n",
        "# -----------------------------------\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_photo, real_sketch):\n",
        "    # persistent=True allows us to calculate multiple gradients\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        # --- Forward Pass ---\n",
        "        fake_sketch = gen_G(real_photo, training=True)\n",
        "        fake_photo = gen_F(real_sketch, training=True)\n",
        "\n",
        "        cycled_photo = gen_F(fake_sketch, training=True)\n",
        "        cycled_sketch = gen_G(fake_photo, training=True)\n",
        "\n",
        "        same_photo = gen_F(real_photo, training=True)\n",
        "        same_sketch = gen_G(real_sketch, training=True)\n",
        "\n",
        "        disc_real_photo = disc_X(real_photo, training=True)\n",
        "        disc_fake_photo = disc_X(fake_photo, training=True)\n",
        "\n",
        "        disc_real_sketch = disc_Y(real_sketch, training=True)\n",
        "        disc_fake_sketch = disc_Y(fake_sketch, training=True)\n",
        "\n",
        "        # --- Calculate Losses ---\n",
        "        # 1. Generator losses\n",
        "        gen_g_loss = generator_loss(disc_fake_sketch) # G wants to fool D_Y\n",
        "        gen_f_loss = generator_loss(disc_fake_photo) # F wants to fool D_X\n",
        "\n",
        "        # 2. Cycle-consistency losses\n",
        "        total_cycle_loss = cycle_loss(real_photo, cycled_photo) + cycle_loss(real_sketch, cycled_sketch)\n",
        "\n",
        "        # 3. Identity losses\n",
        "        total_identity_loss = identity_loss(real_photo, same_photo) + identity_loss(real_sketch, same_sketch)\n",
        "\n",
        "        # Total Generator Loss\n",
        "        total_gen_g_loss = gen_g_loss + total_cycle_loss + total_identity_loss\n",
        "        total_gen_f_loss = gen_f_loss + total_cycle_loss + total_identity_loss\n",
        "\n",
        "        # 4. Discriminator losses\n",
        "        disc_x_loss = discriminator_loss(disc_real_photo, disc_fake_photo)\n",
        "        disc_y_loss = discriminator_loss(disc_real_sketch, disc_fake_sketch)\n",
        "\n",
        "    # --- Calculate and Apply Gradients ---\n",
        "    generator_g_gradients = tape.gradient(total_gen_g_loss, gen_G.trainable_variables)\n",
        "    generator_f_gradients = tape.gradient(total_gen_f_loss, gen_F.trainable_variables)\n",
        "\n",
        "    discriminator_x_gradients = tape.gradient(disc_x_loss, disc_X.trainable_variables)\n",
        "    discriminator_y_gradients = tape.gradient(disc_y_loss, disc_Y.trainable_variables)\n",
        "\n",
        "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, gen_G.trainable_variables))\n",
        "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, gen_F.trainable_variables))\n",
        "\n",
        "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients, disc_X.trainable_variables))\n",
        "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients, disc_Y.trainable_variables))\n",
        "\n",
        "    # --- THIS IS THE PART THAT WAS LIKELY MISSING ---\n",
        "    return {\n",
        "        \"gen_g_loss\": total_gen_g_loss,\n",
        "        \"gen_f_loss\": total_gen_f_loss,\n",
        "        \"disc_x_loss\": disc_x_loss,\n",
        "        \"disc_y_loss\": disc_y_loss,\n",
        "        \"cycle_loss\": total_cycle_loss,\n",
        "        \"identity_loss\": total_identity_loss\n",
        "    }\n",
        "\n",
        "print(\"Train step function (re)defined.\")"
      ],
      "metadata": {
        "id": "6SGmKjUzHurf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983f73ea-3b49-4a87-da34-1638f8260a3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train step function (re)defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Cell 9: The Training Loop\n",
        "# -----------------------------\n",
        "\n",
        "EPOCHS = 3 # You can start with 10-20, then increase\n",
        "\n",
        "print(f\"Starting training for {EPOCHS} epochs...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    n = 0\n",
        "    total_g_loss = 0\n",
        "    total_d_loss = 0\n",
        "\n",
        "    for (photo, sketch) in train_dataset:\n",
        "        losses = train_step(photo, sketch)\n",
        "        if n % 200 == 0:\n",
        "            print(f\"  Batch {n}: G_loss={losses['gen_g_loss']:.4f}, D_X_loss={losses['disc_x_loss']:.4f}\")\n",
        "\n",
        "        total_g_loss += losses['gen_g_loss']\n",
        "        total_d_loss += (losses['disc_x_loss'] + losses['disc_y_loss']) / 2\n",
        "        n += 1\n",
        "\n",
        "    # Save a checkpoint every epoch\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "    avg_g_loss = total_g_loss / n\n",
        "    avg_d_loss = total_d_loss / n\n",
        "\n",
        "    print(f'Time for epoch {epoch + 1} is {time.time()-start:.2f} sec')\n",
        "    print(f'  Average G_Loss: {avg_g_loss:.4f}, Average D_Loss: {avg_d_loss:.4f}\\n')\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "9FNZtVvRHw4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b186b54d-bb55-4936-f896-7e58e6de0b96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 3 epochs...\n",
            "  Batch 0: G_loss=2.1811, D_X_loss=0.0278\n",
            "  Batch 200: G_loss=2.2780, D_X_loss=0.0604\n",
            "  Batch 400: G_loss=3.3077, D_X_loss=0.1388\n",
            "  Batch 600: G_loss=2.2058, D_X_loss=0.3100\n",
            "  Batch 800: G_loss=2.4485, D_X_loss=0.0781\n",
            "  Batch 1000: G_loss=2.0018, D_X_loss=0.1721\n",
            "  Batch 1200: G_loss=2.0849, D_X_loss=0.0680\n",
            "  Batch 1400: G_loss=2.4529, D_X_loss=0.2373\n",
            "  Batch 1600: G_loss=2.8073, D_X_loss=0.1018\n",
            "  Batch 1800: G_loss=3.7866, D_X_loss=0.1597\n",
            "  Batch 2000: G_loss=2.1221, D_X_loss=0.2265\n",
            "  Batch 2200: G_loss=1.5943, D_X_loss=0.1148\n",
            "  Batch 2400: G_loss=2.5869, D_X_loss=0.3096\n",
            "  Batch 2600: G_loss=1.9985, D_X_loss=0.0639\n",
            "  Batch 2800: G_loss=2.5654, D_X_loss=0.1235\n",
            "  Batch 3000: G_loss=1.5866, D_X_loss=0.1247\n",
            "  Batch 3200: G_loss=2.1496, D_X_loss=0.0786\n",
            "  Batch 3400: G_loss=1.9774, D_X_loss=0.0400\n",
            "  Batch 3600: G_loss=1.9483, D_X_loss=0.1340\n",
            "  Batch 3800: G_loss=2.2799, D_X_loss=0.1542\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/Colab_CheckPoints/CycleGAN/train/ckpt-13\n",
            "Time for epoch 1 is 2005.65 sec\n",
            "  Average G_Loss: 2.3091, Average D_Loss: 0.1304\n",
            "\n",
            "  Batch 0: G_loss=2.0041, D_X_loss=0.1058\n",
            "  Batch 200: G_loss=2.3736, D_X_loss=0.1256\n",
            "  Batch 400: G_loss=2.3358, D_X_loss=0.0622\n",
            "  Batch 600: G_loss=2.8791, D_X_loss=0.1899\n",
            "  Batch 800: G_loss=2.2826, D_X_loss=0.1112\n",
            "  Batch 1000: G_loss=2.3282, D_X_loss=0.2128\n",
            "  Batch 1200: G_loss=2.3466, D_X_loss=0.1062\n",
            "  Batch 1400: G_loss=2.0657, D_X_loss=0.1862\n",
            "  Batch 1600: G_loss=1.8203, D_X_loss=0.0956\n",
            "  Batch 1800: G_loss=2.3714, D_X_loss=0.0649\n",
            "  Batch 2000: G_loss=2.2185, D_X_loss=0.1356\n",
            "  Batch 2200: G_loss=2.7711, D_X_loss=0.0599\n",
            "  Batch 2400: G_loss=2.3657, D_X_loss=0.0966\n",
            "  Batch 2600: G_loss=2.7155, D_X_loss=0.0654\n",
            "  Batch 2800: G_loss=1.6808, D_X_loss=0.0501\n",
            "  Batch 3000: G_loss=2.1446, D_X_loss=0.1845\n",
            "  Batch 3200: G_loss=2.8874, D_X_loss=0.0531\n",
            "  Batch 3400: G_loss=2.7715, D_X_loss=0.1554\n",
            "  Batch 3600: G_loss=1.7845, D_X_loss=0.1592\n",
            "  Batch 3800: G_loss=2.0107, D_X_loss=0.0800\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/Colab_CheckPoints/CycleGAN/train/ckpt-14\n",
            "Time for epoch 2 is 1842.98 sec\n",
            "  Average G_Loss: 2.3325, Average D_Loss: 0.1265\n",
            "\n",
            "  Batch 0: G_loss=2.2911, D_X_loss=0.1198\n",
            "  Batch 200: G_loss=2.6783, D_X_loss=0.0865\n",
            "  Batch 400: G_loss=2.6487, D_X_loss=0.1116\n",
            "  Batch 600: G_loss=2.1143, D_X_loss=0.0848\n",
            "  Batch 800: G_loss=2.4178, D_X_loss=0.0883\n",
            "  Batch 1000: G_loss=3.6741, D_X_loss=0.0523\n",
            "  Batch 1200: G_loss=2.1812, D_X_loss=0.0896\n",
            "  Batch 1400: G_loss=2.1001, D_X_loss=0.0725\n",
            "  Batch 1600: G_loss=1.5668, D_X_loss=0.0486\n",
            "  Batch 1800: G_loss=3.3236, D_X_loss=0.1313\n",
            "  Batch 2000: G_loss=3.2983, D_X_loss=0.2772\n",
            "  Batch 2200: G_loss=2.6048, D_X_loss=0.1244\n",
            "  Batch 2400: G_loss=2.5275, D_X_loss=0.1012\n",
            "  Batch 2600: G_loss=1.7067, D_X_loss=0.0781\n",
            "  Batch 2800: G_loss=3.0026, D_X_loss=0.1967\n",
            "  Batch 3000: G_loss=2.0263, D_X_loss=0.0859\n",
            "  Batch 3200: G_loss=2.4636, D_X_loss=0.0551\n",
            "  Batch 3400: G_loss=2.2898, D_X_loss=0.0466\n",
            "  Batch 3600: G_loss=1.9653, D_X_loss=0.1936\n",
            "  Batch 3800: G_loss=2.7160, D_X_loss=0.1582\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/Colab_CheckPoints/CycleGAN/train/ckpt-15\n",
            "Time for epoch 3 is 1850.11 sec\n",
            "  Average G_Loss: 2.3446, Average D_Loss: 0.1216\n",
            "\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# Cell 10: Save Final Models\n",
        "# ---------------------------------\n",
        "\n",
        "print(\"Saving final generator models...\")\n",
        "\n",
        "# Save Photo-to-Sketch generator\n",
        "gen_G.save('photo_to_sketch_generator.h5')\n",
        "\n",
        "# Save Sketch-to-Photo generator\n",
        "gen_F.save('sketch_to_photo_generator.h5')\n",
        "\n",
        "print(\"Models saved as photo_to_sketch_generator.h5 and sketch_to_photo_generator.h5\")"
      ],
      "metadata": {
        "id": "DMgHMUaJHy9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d909e91-0257-41b3-b0ee-0a92b51eb225"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving final generator models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models saved as photo_to_sketch_generator.h5 and sketch_to_photo_generator.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# Cell 11: Write Flask App (app.py)\n",
        "# ---------------------------------\n",
        "\n",
        "%%writefile app.py\n",
        "\n",
        "import flask\n",
        "from flask import Flask, request, render_template, jsonify\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import warnings\n",
        "\n",
        "# Suppress TensorFlow warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# --- Model Loading ---\n",
        "# We no longer need custom_objects because GroupNormalization is standard\n",
        "print(\"Loading models...\")\n",
        "try:\n",
        "    gen_PhotoToSketch = tf.keras.models.load_model('photo_to_sketch_generator.h5', compile=False)\n",
        "    gen_SketchToPhoto = tf.keras.models.load_model('sketch_to_photo_generator.h5', compile=False)\n",
        "    print(\"Models loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models: {e}\")\n",
        "    # Create dummy functions if models aren't found\n",
        "    gen_PhotoToSketch = lambda x: x\n",
        "    gen_SketchToPhoto = lambda x: x\n",
        "\n",
        "# --- Helper Functions ---\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "\n",
        "def preprocess_image(img_bytes):\n",
        "    # Decode image\n",
        "    img = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    # Auto-detect logic\n",
        "    is_sketch = is_image_a_sketch(img_np)\n",
        "\n",
        "    # Preprocess for model\n",
        "    img_tensor = tf.convert_to_tensor(img_np, dtype=tf.float32)\n",
        "    img_tensor = tf.image.resize(img_tensor, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    img_tensor = (img_tensor / 127.5) - 1 # Normalize to [-1, 1]\n",
        "    img_tensor = tf.expand_dims(img_tensor, 0) # Add batch dimension\n",
        "\n",
        "    return img_tensor, is_sketch\n",
        "\n",
        "def is_image_a_sketch(img, saturation_threshold=20):\n",
        "    \"\"\"\n",
        "    Auto-detects if an image is a sketch or a real photo.\n",
        "    Heuristic: Sketches are grayscale or have very low color saturation.\n",
        "    \"\"\"\n",
        "    if len(img.shape) == 2: # It's already grayscale\n",
        "        return True\n",
        "\n",
        "    # Convert to HSV color space\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "    # Calculate average saturation (S channel)\n",
        "    avg_saturation = np.mean(hsv[:, :, 1])\n",
        "\n",
        "    print(f\"Average Saturation: {avg_saturation}\")\n",
        "    return avg_saturation < saturation_threshold\n",
        "\n",
        "def postprocess_image(tensor):\n",
        "    # De-normalize from [-1, 1] to [0, 255]\n",
        "    tensor = (tensor * 0.5 + 0.5) * 255\n",
        "    img = tf.cast(tensor, tf.uint8).numpy()\n",
        "    return img\n",
        "\n",
        "# --- Flask Routes ---\n",
        "@app.route('/')\n",
        "def index():\n",
        "    # Render the main HTML page\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file part'})\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No selected file'})\n",
        "\n",
        "    if file:\n",
        "        try:\n",
        "            img_bytes = file.read()\n",
        "\n",
        "            # 1. Preprocess and Auto-Detect\n",
        "            input_tensor, is_sketch = preprocess_image(img_bytes)\n",
        "\n",
        "            # 2. Run inference\n",
        "            if is_sketch:\n",
        "                print(\"Detected sketch, converting to photo...\")\n",
        "                output_tensor = gen_SketchToPhoto(input_tensor, training=False)\n",
        "            else:\n",
        "                print(\"Detected photo, converting to sketch...\")\n",
        "                output_tensor = gen_PhotoToSketch(input_tensor, training=False)\n",
        "\n",
        "            # 3. Post-process\n",
        "            output_img = postprocess_image(output_tensor[0]) # Remove batch dim\n",
        "\n",
        "            # 4. Encode as Base64 to send back to browser\n",
        "            # Convert RGB (from TF) to BGR (for cv2.imencode)\n",
        "            output_img_bgr = cv2.cvtColor(output_img, cv2.COLOR_RGB2BGR)\n",
        "            _, img_encoded = cv2.imencode('.jpg', output_img_bgr)\n",
        "            img_base64 = base64.b64encode(img_encoded).decode('utf-8')\n",
        "\n",
        "            return jsonify({'image': f'data:image/jpeg;base64,{img_base64}'})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error: {e}\")\n",
        "            return jsonify({'error': 'Failed to process image.'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "-M7RDlpqHzra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e629055c-0657-4a44-9452-fddff9be4ea6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# STEP 1: Mount your Google Drive\n",
        "# ---------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "j-thzum3OiXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ed39bb-273c-4530-e4c9-f29708b93bed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# STEP 2: Copy the files\n",
        "# ---------------------------------\n",
        "print(\"Copying checkpoints to Google Drive... DO NOT DISCONNECT.\")\n",
        "!cp -r ./checkpoints/train /content/drive/MyDrive/my_cyclegan_checkpoints\n",
        "print(\"--- COPY COMPLETE! ---\")"
      ],
      "metadata": {
        "id": "UXIs_EZ1Oy0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd921a9-9592-436f-83ea-484baefd9179"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying checkpoints to Google Drive... DO NOT DISCONNECT.\n",
            "cp: cannot stat './checkpoints/train': No such file or directory\n",
            "--- COPY COMPLETE! ---\n"
          ]
        }
      ]
    }
  ]
}